- 总结:

人生在世不称心如意之事十之八九,这个我懂,但是没想到变化来的如此突然,如此之艰难,让人感到窒息,不知道为什么,自己明明要的不多,也相对来说还算努力,为什么要一次一次的变成这种境况.写下这些的时候我是落泪的,或者说最近这几天一直都在落泪,不知道为什么总是自己前路迷茫?悲伤总是无用的,没有人会在乎你,既然只能靠自己,既然前方已经没有路了,那就只能往前走,不然呢?也可以允许自己悲伤,但是别忘记了努力,因为只有悲伤是没有用的,悲伤不能为你解决任何问题,只能往前一步一步走.
- 准备的问题:
1. 为什么要选择数据产品或策略产品?
- 选择数据的原因是:我对数字或者说用学习到的数据模型去解决实际问题很感兴趣,相信数据能够指导我们的工作,从数据中可以帮我们看到我们的认知偏差
- 想要锻炼自己的性格,使自己具备能够专注于一项事情的能力,所以想选择数据分析

2.自我介绍?

>我叫都书鹏,17年毕业的,工作2年多点,整个工作中主要就是两个方向:
- 处理收集展示数据:埋点规范定义,埋点设计,数据指标的定义和收口,需求报表;埋点平台,数据平台的搭建
- 使用数据:
	- 反作弊:酷划是一款网赚类产品,所以会有薅羊毛用户,我们主要就是识别到相应的作弊用户并且追回损失,我的工作主要是设置相应的业务规则,第二阶段数据的清洗,模型的调参以及最后工具化
	- LTV:计算用户的ROI,使市场投放更加精确化,我的主要工作就是数据的清洗,将埋点数据变成用户宽表以计算LTV,LTV口径的定义(比如徒弟的组成),以及工具化
	- 搜索:包括评价指标的定义,基于业务的策略优化,产品功能优化
	- 也还做过一阵的文章推荐,包括festtest的调研,基于相关推荐的文章召回提升了10%的点击率

3. 数据指标搭建工作如何做的?(搜索策略如何做的?)
>以搜索为例:
- 首先要熟悉我们的业务流程(此处建议先画出整个流程图,然后在具体拆分每一块属于哪个部分),然后定义目标函数f(用户画像,搜索关键词,场景上下文......)=点击率/命中用户需求的概率(是否下单了),值域在[0,1]之间:
	- 流量侧: 用户整个搜索路径的行为监控,比如入口点击(占用用户的情况),联想词的数据(用于评价我们目前的联想词词库和算法优劣),在搜索页面的点击请况(有多少人点击入口但是没有在页面点击即第一步的流失情况,多次点击即搜索结果可能并不满意,需要进一步比较搜索词),不同商家级别展现情况
	- 算法层: 算法的准确率和召回率(在一次搜索纪录中返回的正确结果是?算法召回商品的情况?),搜索页面的二次点击,无结果页面出现情况,词库的命中情况(判断当前逻辑是否有效)
	- 转化层: 通过搜索的下单数据
	- 性能层: 崩溃率,超时情况
- 确定基础指标:
	- 准确率:
		- 搜索有结果
		- 用户二跳的点击情况
		- 二跳落地页的点击情况
		- 算法的准确率
	- 召回率
		- 算法的召回率
	- 无结果情况
		- 无结果的出现频次
		- 搜索词-服务项ID
	- 订单转化率
- 分维度:我们也关心通过app和h5的搜索用户哪一个质量更好,这样类似的问题
	- 来源:App,H5(M站),小程序
	- 二次点击分位置:(更多评价一个商家排序)

4. Logit回归和线性回归的区别:
- Logit回归:是对离散变量做回归,假设的是事件发生的概率与自变量之间服从logit函数,logit汇报的是一个几率(控制其他的变量,当一个变量从1变到2后,它的变化倍数,当没法做控制变量的实验时,这个方法比较好用)
$$
ln(\frac{p}{1-p})=logit p=b_i*x_i
$$
- 线性回归:是连续变量做回归,其中它得出的coef就是边际效应

5. 数据口径的收口工作?
6. 数据平台的搭建?
- 收集需求:报表,查询埋点
- 拆分各个需求点:
	- 报表:拆分成哪几类(广告,积分,文章,用户);在各个文件夹下如何支持用户自定义配置信息(将数据写到表里后按照需求,维度,报表的列名称,时间控件,报表的名称,所属品类);最后输出报表
	- 查询埋点:主要是调研了神策的数据平台,根据埋点的规则,先提炼出最重要的三个指标(event,page_name,page_type)做为查询的基本要素(缺少则不能进行查询),支持分维度(用户属性,某一个页面下各个参数的值)以及用户自添加变量来显示数据
- 优化:
	- 更多的和数仓结合,做一些调度管理,以及可以将数仓的评价指标搭建到平台上(调度的及时性,数据的使用情况(数据的冗余度))
	- 往上针对用户还可以做漏斗模型
7. 数据埋点体系如何搭建的?
情景就是没有埋点或者目前现有的埋点是混乱不可用的:
- 评价指标:
	- 准确性
	- 全面性(维度,渠道标识)
- 定义公参:网络情况,手机型号
- 梳理主要指标:DAU,注册,登陆,广告点击;通过指标确定埋点页面,根据命名规则指定相应的名字
- 维度的问题:除公参外,针对不同的指标需要细化各个维度,此处需要人工定制
- 优化:自动化埋点;对于一些常规埋点能否抽成工具化,并支持自己配置简单的维度
- 角色:产品角色:包括前期的确定需求,整理计算口径,以及排期的安排;开发测试过程中和相关人员的对接工作;上线结束后,对于使用人员的疑问解答,提升用户体验
8. 如何看待重复性的工作?

	首先工作久了就是会有很多事情都是重复劳动,比如搭建数据平台,你在a公司做了,在b公司可能也得做,结果是一样的,但是不同的是在这个过程中你可以把它做的不一样,加入新的想法以及优化.

9. 职业规划?

- 5年之内可以在数据产品方面有一个长足的进展:具备将常用数据工具抽象化的能力,希望能多了解一下;数据治理(数仓规划,埋点)学习一下别人的经验;培养一定的数据分析思维,掌握常见的分析方法,因为从自身角度来说对学习数学,如何应用数学很感兴趣;所以比较理想的工作内容就是可以将埋点,数仓和做一些策略的工作能够结合起来,这样通过埋点这一块能对我们的数据有跟深入的了解,通过策略这一块能够真正的使用数据.
- 策略:像上面说的一方面是指一些常见的数据分析的工作,能够培养一定的这样的思维;另一个方面做一些和算法有关系的工作,能够有机会多接触一些,包括以前做的内容推荐和搜索都是这方面有关系的(也许离得比较远)
9.1 数据产品的定义?
- 数据服务类,比如功能化的像是抽象各种平台;数仓治理,埋点等面向底层数据的;数据应用:数据产出以后基于我们的算法或者业务规则来使用数据,做一些策略方面的问题;我个人对于纯平台功能化的感觉一般般,更想偏重于纯数据的或者数据使用的方面
- 用户产品就是服务用户,数据产品就是服务数据的
![](https://jiaozi-oss.oss-cn-hongkong.aliyuncs.com/img/WechatIMG1.jpeg)

10. 反作弊的工作
>我们是一款网赚类产品,主要是有一些薅羊毛的用户,目的就是为了识别出他们并在一定程度上看能否追回损失
- 前期:主要是业务逻辑与规则结合起来
	- 同一个作弊师傅所收的徒弟的个数以及每个徒弟贡献的金币数具有高度相似性
	- 加入了埋点特征,看一个师傅下面所有徒弟的点击行为具有相似性,也就是说作弊的人收的徒弟的点击行为给我们传递的信息量少,因此就引入了信息熵的模型,将徒弟点击行为信息熵过小的师傅和徒弟拉灰
	- 一个师傅下面可能有三种不同类型的点击行为的徒弟(10次新闻点击的徒弟6人;10次广告点击的徒弟10人;12次Tab点击的徒弟15人),设定阈值后,将这些特征提取,命中这些特征的师傅徒弟拉灰
- 后期:引入了SVM和统计学模型:
  - 特征:用用户的静态行为(邀请人,积分,版本,设备,IP,城市,电话话段),业务数据(徒弟贡献金币),埋点(徒弟点击行为的md5)
  - 去除特征全部相同或者全部不同的(userid)
  - 粗加工:phone_number大部分并不相同,可以选择只保留到前7位;ctime化成精确到秒或者小时
  - $Y=\omega_1*x_1+\omega_2*x_2+.....+\omega_n*x_n$,我们定义$\omega$是数据收敛性的权重,也就是越收敛权重越大,越是个灰用户,自然想到要用熵来定义权重,但是不同特征的比例尺不一样,联想到变异系数,所以引入$\omega=\frac{信息熵}{最大信息熵}$,特征的不重复个数越多或者越少我们都应该约束一下,所以引入乘数因子即特征不重复的个数(这个因子,大于10的取10,小于3取3,在中间进行过标准化),最后得到的$\omega$在[0,1]之间的,所以要给权重加1
  - 给特征化成相应的数值,eg:对某一个城市的黑用户的城市特征评分用这个城市黑用户的概率经过一系列标准化最后得到的数值
- 工作:
	- 前期通过观察数据,制定策略并不断有优化的过程(包括引入熵的概念),实现部署
	- 后期就是数据的清洗,模型调参,特征评分
	- 把控整个项目的进展流程,加上半个rd的工作
- 问题:
	- 特征数据的处理问题:脏数据如何处理?埋点数据如何选取?->引入了熵的概念,熵越大代表随机变量的不确定性就越大,从概率上来讲这个越可能是个正常行为
- 优化:
	- 引入微信特征,当时一个分股的活动区分度还是很大的,业务规则
	- W权重的算法,我们现在经过人工处理以后,权重总是在1以上,可以训练权重,如果某一列权重为负,那它就起反作用,拥有某一种特征它更可能是白用户,这样就体现在了权重之中
- SVM:
	- C越大,正确率提高,泛化能力在降低,更容易过拟合;C越大越退化成线性可分支持向量机
	- 高斯核:$gamma=\frac{1}{方差}$,gamma越大,正确率再提高,泛化能力在下降
	- SVM自带一个L2的正则项(惩罚项),|w|,C其实就是更重视正则项还是更重视损失函数
11.  为什么离职?
- 酷划离职:因为自己确定想做数据产品经理偏策略方向,而当前的职位提供不了这样的机会所以决定离职
- 五八到家离职:因为目前组织架构的调整,导致部门解散,后续会被划分给做用户端产品,离数据越来越远了,跟自己的职业规划不符,决定离职
12. 优点和缺点?
- 优点:
  - 从做产品的角度来说我个人的性格是比较往前冲,适合打头阵的人
  - 我个人比较喜欢理科,喜欢数学,物理这些东西,所以想做跟数据的工作,综合来看是我选择数据产品的原因 
- 缺点:
  - 我个人说话语速比较快,不论是面试还是平时给人感觉就不太好,平时有努力读课文在调整这个状态 
13. 项目中你的角色?你对于数据准确性的确定?
14. 搜索策略如何做?
- 搭建指标体系
- 近义词库:
	- 用处:ik分词器,联想词匹配
	- 来源:爬虫对应的关键词;用户搜索无结果-相关推荐词的点击;运营帮忙整理的文档
- 从各项指标入手:
	- 无结果率:无结果有推荐词;无结果有推荐列表;纯无结果;纯无结果:
		- 用户搜索词和我们的词库未匹配上,增加词库;引入文章内容,再从用户点击的文章分类入手去获得对应关系从而提高搜索准确率
		- 业务上的规则,比如搜索阿姨,引入阿姨端的界面;找工作,引入职培
	- 准确率:商家排名
15. LTV?

- 公式:$LTV=(每日的平均arpu-每日的平均成本)*LT-获客成本$
- 拆解主要指标和纬度:
指标|已发生|未发生|每日的平均量
--|--|--|--
LT|$一批用户从注册到开始计算的留存率记作这个用户的已发生LT$|$总LT-已发生LT$|$已知这批用户的留存情况带入幂函数进行拟合,得出的面积既是总LT$
arpu|$(\Sigma_i^n日广告i点击量*广告i的单价)*已发生LT$|$(\Sigma_i^n预测日广告i点击量*广告i的单价)*未发生LT$|$\frac{已发生Arpu*已发生LT+预估Arpu*未发生LT}{LT}$
成本|$获客成本+已发生维护成本$|$预测日维护成本*未发生LT$|	$\frac{已发生维护成本*已发生LT+预估维护成本*未发生LT}{LT}$
- 细节:广告单价:总收入/总点击
- 工作:
  - 底层数据处理,将各个来源的数据整合并输出宽表
  - 制定了LTV的总体计算框架
  - 根据框架和各方的协助(数据组那得到折扣率和兑换率,上午那得来的收入数据),通过python web实现整个功能
- 优化:
  - 认为广告单价是恒定不变的,这个其实需要考虑,可以采取单价是动态的
1.  你最大的收获?

	最开始做埋点的时候,从什么是埋点到埋点的作用,如何设计埋点做这一套,最后以Excel的格式进行传阅相关业务埋点.随着业务体量的增大,以及埋点越来越复杂,和一些人员流动的关系,用Excel进行埋点记载变得越来越不现实,跟我们当时的领导商量以后,从0到1开始设计埋点平台,实现埋点从设计,开发到检测的流程化.第一次自己主导一个项目的经历(如何管控时间节点,如何去跟各方对接,协商资源)对我是很重要的,并且也学会了在工作中需要不断思考,不断总结.

17. word2vec的原理?
    1.  词向量训练:采用霍夫曼树进行训练
	![](https://jiaozi-oss.oss-cn-hongkong.aliyuncs.com/img/霍夫曼树.png)  
	约定左子树 编码为1,右子树编码为0;a:00;b:1110;c:11;e:01;f:1001
1.  工作中遇到最大的挫败?
2.  对工作的这个行业要求?对工作的要求?
3.  统计连续活跃天数?(现在有一张表t，这张表存储了每个员工每天的打卡情况，现在需要统计截止目前每个员工的连续打卡天数)
```sql
select userid, ((unix_timestamp(当前日期,'yyyyMMdd') - unix_timestamp(max(log_day),'yyyyMMdd'))/86400) as days from 
(select userid,is_active,log_day,ROW_NUMBER() OVER(PARTITION BY userid,is_active  ORDER BY log_day desc) as rank 
                from tmp.tmp_activity_active where userid='48301150'
                having is_active=0
)g
group by userid
```
